{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/62781349/how-to-use-neuralcoref-in-spacy\n",
    "# BE CAREFUL\n",
    "# Set your environment with\n",
    "# Python = 3.7\n",
    "# spacy = 2.1.0\n",
    "# neuralcoref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en_core_web_md==2.1.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_md-2.1.0/en_core_web_md-2.1.0.tar.gz (95.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.4/95.4 MB\u001b[0m \u001b[31m52.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the model via spacy.load('en_core_web_md')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import neuralcoref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load SpaCy's English model\n",
    "nlp = spacy.load('en_core_web_md')\n",
    "\n",
    "# Add neuralcoref to SpaCy's pipeline\n",
    "neuralcoref.add_to_pipe(nlp)\n",
    "\n",
    "# Input text for coreference resolution\n",
    "text = \"\"\"John asked Mary to go out. She said she was busy. John was disappointed but understood.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the text with SpaCy\n",
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coreference cluster: John: [John, John]\n",
      "Coreference cluster: Mary: [Mary, She, she]\n",
      "\n",
      "Resolved document:\n",
      "John asked Mary to go out. Mary said Mary was busy. John was disappointed but understood.\n"
     ]
    }
   ],
   "source": [
    "# Print coreference clusters\n",
    "if doc._.has_coref:\n",
    "    for cluster in doc._.coref_clusters:\n",
    "        print(f\"Coreference cluster: {cluster}\")\n",
    "\n",
    "# Print the resolved text\n",
    "resolved_text = doc._.coref_resolved\n",
    "print(\"\\nResolved document:\")\n",
    "print(resolved_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<spacy.lang.en.English at 0x7f8a202d04c0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neuralcoref.add_to_pipe(nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Rihanna is basically master of the fashion universe right now, so we're naturally going to pay attention to what trends she is and isn't wearing whenever she steps out of the door (or black SUV). She's having quite the epic week, first presenting her Savage x Fenty lingerie runway show then hosting her annual Diamond Ball charity event last night. Rihanna was decked out in Givenchy for the big event, but upon arrival at the venue, she wore a T-shirt, diamonds (naturally), and a scarf, leather pants, and heels in fall's biggest color trend: pistachio green.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc._.has_coref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cluster in doc._.coref_clusters:\n",
    "    for reference in cluster:\n",
    "    #each of these is a Span object in Spacy\n",
    "        print(reference)\n",
    "        #starting index of this reference in the text\n",
    "        print(reference.start) \n",
    "        #ending index of this reference in the text\n",
    "        print(reference.end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resolved_doc = doc._.coref_resolved\n",
    "print(resolved_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<spacy.lang.en.English at 0x7fd9903249a0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json, random\n",
    "from tqdm.notebook import tqdm\n",
    "from urllib.parse import quote\n",
    "\n",
    "# Load your usual SpaCy model (one of SpaCy English models)\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Add neural coref to SpaCy's pipe\n",
    "import neuralcoref\n",
    "neuralcoref.add_to_pipe(nlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"When Sebastian Thrun started working on self-driving cars at Google in 2007, few people outside of the company took him seriously.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "displacy.render(doc, style=\"ent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coreference Resolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text2 = 'Angela lives in Boston. She is quite happy in that city.'\n",
    "doc = nlp(text2)\n",
    "\n",
    "for ent in doc.ents:\n",
    "    print(ent._.coref_cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import neuralcoref\n",
    "\n",
    "# nlp = spacy.load('en')\n",
    "# neuralcoref.add_to_pipe(nlp)\n",
    "doc1 = nlp('My sister has a dog. She loves him.')\n",
    "print(doc1._.coref_clusters)\n",
    "\n",
    "doc2 = nlp('Angela lives in Boston. She is quite happy in that city.')\n",
    "for ent in doc2.ents:\n",
    "    print(ent._.coref_cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in the coref model\n",
    "# You can download the model here\n",
    "# https://github.com/explosion/spacy-experimental/releases/tag/v0.6.0\n",
    "# nlp = spacy.load('en')\n",
    "nlp = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process example sentence\n",
    "# Other examples:\n",
    "# Philip plays the bass because he loves it.\n",
    "# Sarah enjoys a nice cup of tea in the morning. She likes it with sugar and a drop of milk.\n",
    "# John said hi. Big old John is always around.\n",
    "doc = nlp(\"Philip plays the bass because he loves it.\")\n",
    "# doc = nlp(\"Sarah enjoys a nice cup of tea in the morning. She likes it with sugar and a drop of milk.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "for ent in doc.ents:\n",
    "    print(ent._.coref_cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Pipeline components\u001b[0m\n",
      "0: tagger\n",
      "1: parser\n",
      "2: ner\n",
      "\u001b[38;5;4mℹ Found clusters\u001b[0m\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'spacy.tokens.doc.Doc' object has no attribute 'spans'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [8]\u001b[0m, in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Print out clusters\u001b[39;00m\n\u001b[1;32m      7\u001b[0m msg\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound clusters\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m cluster \u001b[38;5;129;01min\u001b[39;00m \u001b[43mdoc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mspans\u001b[49m:\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcluster\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdoc\u001b[38;5;241m.\u001b[39mspans[cluster]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'spacy.tokens.doc.Doc' object has no attribute 'spans'"
     ]
    }
   ],
   "source": [
    "# Print out component names\n",
    "msg.info(\"Pipeline components\")\n",
    "for i, pipe in enumerate(nlp.pipe_names):\n",
    "    print(f\"{i}: {pipe}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Found clusters\u001b[0m\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'spacy.tokens.doc.Doc' object has no attribute 'spans'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [9]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Print out clusters\u001b[39;00m\n\u001b[1;32m      2\u001b[0m msg\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound clusters\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m cluster \u001b[38;5;129;01min\u001b[39;00m \u001b[43mdoc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mspans\u001b[49m:\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcluster\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdoc\u001b[38;5;241m.\u001b[39mspans[cluster]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'spacy.tokens.doc.Doc' object has no attribute 'spans'"
     ]
    }
   ],
   "source": [
    "# Print out clusters\n",
    "msg.info(\"Found clusters\")\n",
    "for cluster in doc.spans:\n",
    "    print(f\"{cluster}: {doc.spans[cluster]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define lightweight function for resolving references in text\n",
    "def resolve_references(doc: Doc) -> str:\n",
    "    \"\"\"Function for resolving references with the coref ouput\n",
    "    doc (Doc): The Doc object processed by the coref pipeline\n",
    "    RETURNS (str): The Doc string with resolved references\n",
    "    \"\"\"\n",
    "    # token.idx : token.text\n",
    "    token_mention_mapper = {}\n",
    "    output_string = \"\"\n",
    "    clusters = [\n",
    "        val for key, val in doc.spans.items() if key.startswith(\"coref_cluster\")\n",
    "    ]\n",
    "\n",
    "    # Iterate through every found cluster\n",
    "    for cluster in clusters:\n",
    "        first_mention = cluster[0]\n",
    "        # Iterate through every other span in the cluster\n",
    "        for mention_span in list(cluster)[1:]:\n",
    "            # Set first_mention as value for the first token in mention_span in the token_mention_mapper\n",
    "            token_mention_mapper[mention_span[0].idx] = first_mention.text + mention_span[0].whitespace_\n",
    "            \n",
    "            for token in mention_span[1:]:\n",
    "                # Set empty string for all the other tokens in mention_span\n",
    "                token_mention_mapper[token.idx] = \"\"\n",
    "\n",
    "    # Iterate through every token in the Doc\n",
    "    for token in doc:\n",
    "        # Check if token exists in token_mention_mapper\n",
    "        if token.idx in token_mention_mapper:\n",
    "            output_string += token_mention_mapper[token.idx]\n",
    "        # Else add original token text\n",
    "        else:\n",
    "            output_string += token.text + token.whitespace_\n",
    "\n",
    "    return output_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Document with resolved references\u001b[0m\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'spacy.tokens.doc.Doc' object has no attribute 'spans'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [8]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m msg\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDocument with resolved references\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m processed_doc \u001b[38;5;241m=\u001b[39m \u001b[43mresolve_references\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(resolve_references(doc))\n",
      "Input \u001b[0;32mIn [7]\u001b[0m, in \u001b[0;36mresolve_references\u001b[0;34m(doc)\u001b[0m\n\u001b[1;32m      8\u001b[0m token_mention_mapper \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m      9\u001b[0m output_string \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     10\u001b[0m clusters \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m---> 11\u001b[0m     val \u001b[38;5;28;01mfor\u001b[39;00m key, val \u001b[38;5;129;01min\u001b[39;00m \u001b[43mdoc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mspans\u001b[49m\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m key\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcoref_cluster\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     12\u001b[0m ]\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Iterate through every found cluster\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m cluster \u001b[38;5;129;01min\u001b[39;00m clusters:\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'spacy.tokens.doc.Doc' object has no attribute 'spans'"
     ]
    }
   ],
   "source": [
    "msg.info(\"Document with resolved references\")\n",
    "processed_doc = resolve_references(doc)\n",
    "print(resolve_references(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import urllib.request\n",
    "# from bs4 import BeautifulSoup\n",
    "# import spacy\n",
    "# import neuralcoref\n",
    "# nlp = spacy.load('en_core_web_lg')\n",
    "# neuralcoref.add_to_pipe(nlp)\n",
    "\n",
    "# # html = urllib.request.urlopen('https://www.law.cornell.edu/supremecourt/text/418/683').read()\n",
    "\n",
    "# html = urllib.request.urlopen('https://www.nbcnews.com/business/business-news/biden-preparing-block-us-steel-sale-japanese-company-rcna169595').read()\n",
    "\n",
    "# soup = BeautifulSoup(html, 'html.parser')\n",
    "# text = ''.join([t for t in soup.find_all(text=True) if t.parent.name == 'p' and len(t) >= 25])\n",
    "# doc = nlp(text)\n",
    "# resolved_text = doc._.coref_resolved\n",
    "# sentences = [sent.string.strip() for sent in nlp(resolved_text).sents]\n",
    "# output = [sent for sent in sentences if 'president' in \n",
    "#           (' '.join([token.lemma_.lower() for token in nlp(sent)]))]\n",
    "# print('Fact count:', len(output))\n",
    "# for fact in range(len(output)):\n",
    "#     print(str(fact+1)+'.', output[fact])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 5 files: 100%|██████████| 5/5 [00:40<00:00,  8.02s/it]\n",
      "/Users/minjoo/opt/anaconda3/envs/hanhwa/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "/Users/minjoo/opt/anaconda3/envs/hanhwa/lib/python3.8/site-packages/transformers/convert_slow_tokenizer.py:551: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from gliner import GLiNER\n",
    "\n",
    "# Initialize GLiNER with the base model\n",
    "# model = GLiNER.from_pretrained(\"urchade/gliner_mediumv2.1\")\n",
    "model = GLiNER.from_pretrained(\"urchade/gliner_small-v2.1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Labels for entity prediction\n",
    "# Most GLiNER models should work best when entity types are in lower case or title case\n",
    "labels = [\"Person\", \"Award\", \"Date\", \"Competitions\", \"Teams\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform entity prediction\n",
    "entities = model.predict_entities(processed_doc, labels, threshold=0.5)\n",
    "\n",
    "# Display predicted entities and their labels\n",
    "for entity in entities:\n",
    "    print(entity[\"text\"], \"=>\", entity[\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample text for entity prediction\n",
    "text = \"\"\"\n",
    "Cristiano Ronaldo dos Santos Aveiro (Portuguese pronunciation: [kɾiʃˈtjɐnu ʁɔˈnaldu]; born 5 February 1985) is a Portuguese professional footballer who plays as a forward for and captains both Saudi Pro League club Al Nassr and the Portugal national team. Widely regarded as one of the greatest players of all time, Ronaldo has won five Ballon d'Or awards,[note 3] a record three UEFA Men's Player of the Year Awards, and four European Golden Shoes, the most by a European player. He has won 33 trophies in his career, including seven league titles, five UEFA Champions Leagues, the UEFA European Championship and the UEFA Nations League. Ronaldo holds the records for most appearances (183), goals (140) and assists (42) in the Champions League, goals in the European Championship (14), international goals (128) and international appearances (205). He is one of the few players to have made over 1,200 professional career appearances, the most by an outfield player, and has scored over 850 official senior career goals for club and country, making him the top goalscorer of all time.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Labels for entity prediction\n",
    "# Most GLiNER models should work best when entity types are in lower case or title case\n",
    "labels = [\"Person\", \"Award\", \"Date\", \"Competitions\", \"Teams\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cristiano Ronaldo dos Santos Aveiro => Person\n",
      "5 February 1985 => Date\n",
      "Portugal national team => Teams\n",
      "Ballon d'Or => Award\n",
      "UEFA Men's Player of the Year Awards => Award\n",
      "European Golden Shoes => Award\n",
      "UEFA Champions Leagues => Competitions\n",
      "UEFA European Championship => Competitions\n",
      "UEFA Nations League => Competitions\n",
      "European Championship => Competitions\n"
     ]
    }
   ],
   "source": [
    "# Perform entity prediction\n",
    "entities = model.predict_entities(text, labels, threshold=0.5)\n",
    "\n",
    "# Display predicted entities and their labels\n",
    "for entity in entities:\n",
    "    print(entity[\"text\"], \"=>\", entity[\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample text for entity prediction\n",
    "text = \"\"\"\n",
    "David went to the concert. He said it was an amazing experience. \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Labels for entity prediction\n",
    "# Most GLiNER models should work best when entity types are in lower case or title case\n",
    "labels = [\"Person\", \"Award\", \"Date\", \"Competitions\", \"Teams\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/minjoo/opt/anaconda3/envs/hanhwa/lib/python3.8/site-packages/gliner/data_processing/processor.py:269: UserWarning: Sentence of length 434 has been truncated to 384\n",
      "  warnings.warn(f\"Sentence of length {len(tokens)} has been truncated to {max_len}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wörl => Person\n",
      "Wörl => Person\n",
      "1934 => Date\n",
      "1942 => Date\n",
      "Wörl => Person\n",
      "Wörl => Person\n",
      "Wörl => Person\n",
      "Wörl => Person\n"
     ]
    }
   ],
   "source": [
    "# Perform entity prediction\n",
    "entities = model.predict_entities(text, labels, threshold=0.5)\n",
    "\n",
    "# Display predicted entities and their labels\n",
    "for entity in entities:\n",
    "    print(entity[\"text\"], \"=>\", entity[\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hanhwa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
